---
#
# By default, content added below the "---" mark will appear in the home page
# between the top bar and the list of recent posts.
# To change the home page layout, edit the _layouts/home.html file.
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
# <h3 class="blackpar_title">(Models, Training and Inference)</h3>
layout: home
---
<div style="font-family: 'Source Sans Pro', sans-serif; background: url('/2023/images/banner_no_text.png') no-repeat; background-size: cover; user-select: none;">
	<center>
		<h2 class="blackpar_title" ><b>SUMAC</b>: The 5<sup>th</sup> workshop on the analySis, Understanding and proMotion of heritAge Contents</h2><br> 
		<i> <h3 class="blackpar_title"> Advances in machine learning, signal processing, multimodal techniques and human-machine interaction</h3> </i>
		<h4 class="blackpar_title">29 Oct - 3 Nov 2023, Ottawa (Canada) <br> <b>Attendance Mode: TBA</b> </h4>
			<!-- (Ballroom C) and <b>Virtual</b> </h3> -->
	</center>
</div>
<br>
<p>
The fifth version of the SUMAC (analySis, Understanding and proMotion of heritAge Contents) workshop, like its predecessors, focuses on analyzing, processing and valorizing all types of data related to cultural heritage, including tangible and intangible heritage. As stated by UNESCO, cultural heritage provides societies with a wealth of resources inherited from the past, created in the present for the benefit of future generations. 
</p>

<br>

<!--
<div class="alert alert-danger" role="alert">
  <h4>Mentoring sessions announcement</h4>
  <p>
  The deadline for submitting papers to our second version of the Efficient Natural Language and Speech Processing (ENLSP-II) workshop is 25th of September. For that we will be scheduling two mentioring online sessions to answer your questions. Please join us:
  <br>
  <ul>
	<li>Tuesday the 6th of September 2022 from 10PM to 11PM (UTC-04:00)</li>
	<li>Wednesday the 7th of September 2022 from 9AM to 10AM (UTC-04:00)</li>
	<li>Tuesday the 13th of September 2022 from 10PM to 11PM (UTC-04:00): <a href="https://welink.zhumu.com/j/134854021">link</a></li>
	<li>Wednesday the 14th of September 2022 from 9AM to 10AM (UTC-04:00): <a href="https://welink.zhumu.com/j/130263276">link</a></li>
  </ul>
  </p>
</div>

<br>
-->

<h2 class="blackpar_title" id="overview">Overview</h2>
<p>
The digitisation of large quantities of analogue data and the massive production of born-digital documents for many years now provide us with large volumes of varied multimedia data (images, maps, text, video, 3D objects, multi-sensor data, etc.), an important feature of which is that they are cross-domain. ``Cross-domain'' reflects the fact that these data may have been acquired in very different conditions: different acquisition systems, times and points of view (e.g. a 1962 postcard from the Arc de Triomphe vs. a recent street-view acquisition by mobile mapping of the same monument). These data represent an extremely rich heritage that can be exploited in a wide variety of fields, from Social Sciences and Humanities to land use and territorial policies, including smart city, urban planning, smart tourism and culture, creative media and entertainment. In terms of research in computer science and artificial intelligence, they address challenging problems related to the diversity, specificity and volume of the media, the variety of content descriptors (potentially including the time dimension), the veracity of the data, and the different user needs with respect to engaging with this rich material and the extraction of value out of the data. These challenges are reflected in various research topics such as multimodal and mixed media search, automatic content analysis, multimedia linking and recommendation, and big data analysis and visualisation where scientific bottlenecks may be exacerbated by the time dimension -- which also provides topics of interest such as multimodal time series analysis.
</p>
<br>
<!-- Call for Papers -->
<h2 class="blackpar_title" id="call_for_papers">Call for Papers</h2>
The objective of this workshop is to present and discuss the latest and most significant trends in the analysis, structuring and understanding of multimedia contents dedicated to the valorization of heritage, with the emphasis on enabling access to the big data of the past. We welcome research contributions for the following (but not limited to) topics:

<ul>
	<li>Multimedia and cross-domain data search, interlinking and recommendation</li>
	<li>Dating and spatialization of historical data</li>
	<li>Mixed media data access and indexing</li>
	<li>Multi-modal deep learning</li>
	<li>Deep learning in adverse conditions (transfer learning, learning with side information, etc.)</li>
	<li>Multi-modal time series analysis, evolution modelling </li>
	<li>Multi-modal \& multi-temporal data rendering</li>
	<li>Heritage - Building Information Modelling, Art Virtualisation</li>
	<li>HCI / Interfaces for large-scale datasets</li>
	<li>Smart digitisation of massive quantities of data</li>
	<li>Bench-marking, Open Data Movement</li>
	<li>Generative modelling of cultural heritage</li>
	<li>Machine Learning for Cultural Heritage</li>
</ul>
<br>

<h2 class="blackpar_title" id="imp_dates">Important dates</h2>
<ul> 
	<li>Paper submission: July 5, 2023 (AoE)</li>
	<li>Author acceptance notification: July 30, 2023</li>
	<li>Camera-Ready: August 6, 2023</li>
	<li>Workshop date: TBA, during the main conference</li>
</ul>

<h2 class="blackpar_title" id="submission_guide">Submission guidelines</h2>
<br>
<b>Submission format</b> All submissions must be original work not under review at any other workshop, conference, or journal. The workshop will accept papers describing completed work as well as work in progress. One submission format is accepted: full paper, which must follow the formatting guidelines of the main conference ACM MM 2023. Full papers should be from 6 to 8 pages (plus 2 additional pages for the references), encoded as PDF and using the ACM Article Template. For author guidelines and instructions, please visit https://www.acmmm2023.org/instructions/.
<br>
<b>Peer Review and publication in ACM Digital Library</b> Paper submissions must conform with the “double-blind” review policy. All papers will be peer-reviewed by experts in the field, they will receive at least two reviews. Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality. Depending on the number, maturity and topics of the accepted submissions, the work will be presented via oral or poster sessions. The workshop papers will be published in the ACM Digital Library.

<!-- 
We would like to share some fundamental challenges on improving efficiency of pre- trained models and encourage the NeurIPS community to submit their solutions, ideas, and ongoing work concerning data, model, training, and inference efficiency for NLP and speech processing. The scope of this workshop includes, but not limited to, the following topics:

<b>Efficient Pre-Training</b> Pre-training is a very expensive process. Even a small modification to the configuration of the models requires the user to redo pre-training.
<br>
<ul>
	<li>Accelerating the pre-training process</li>
	<li>Continual/Life-long pre-training and adapting pre-trained models to a new domain</li>
	<li>Efficient initialization and hyper-parameter tuning (HPT)</li>
	<li>Better pre-training self-supervised objectives</li>
	<li>Multi-domain pre-training</li>
	<li>Data vs. Scale of pre-trained models</li>
	<li>Pre-training Multimodal (e.g., text–speech) models</li>
	<li>New efficient architectures for pre-trained models</li>
</ul>


<b>Efficient Fine-tuning</b> Fine-tuning large pre-trained models on downstream tasks can be challenging because pre-trained models are very over-parameterized.
<br>
<ul>
	<li>Parameter-efficient tuning solutions to tune only a portion of the entire network (e.g. adapters)</li>
	<li>Efficient prompt-based fine-tuning</li>
	<li>Accelerating the fine-tuning process (e.g. optimizer, and layer-skipping)</li>
	<li>Efficient federated learning for NLP: reduce the communication costs, tackling heterogeneous data, heterogeneous models.</li>
</ul>


<b>Data Efficiency</b> Pre-trained models rely on a huge amount of unlabeled data which makes the training very sample inefficient.
<br>
<ul>
	<li>Sample efficient training, training with less data, few-shot and zero-shot learning</li>
	<li>Sample efficient data-augmentation, identifying which training samples should be augmented</li>
	<li>Data compression, data distillation</li>
	<li>Data selection, how to improve the quality of pre-training data</li>
</ul>

<b>Inference Efficiency</b> How can we reduce the inference time or memory footprint of a trained model for a particular task?
<br>
<ul>
	<li>Neural model compression techniques such as quantization, pruning, layer decomposition and knowledge distillation (KD) for NLP and Speech</li>
	<li>Impact of different compression techniques on the inductive biases learned by the original models</li>
	<li>Combined compression techniques for more efficient NLP and speech models</li>
	<li>Improving efficiency of KD by removing the teacher</li>
	<li>Extreme model compression (high compression ratio) for very large pre-trained language models</li>
</ul>

<b>Special Track) Efficient Graph Learning for NLP</b>
<br>
<ul>
	<li>Automatically transforming natural language into graph-structured data</li>
	<li>Representation learning on multi-relational or heterogeneous graphs</li>
	<li>Learning the mapping between complex data structures, like Graph2Seq, Graph2Tree, Graph2Graph</li>
	<li>Graph learning with pre-trained language models</li>
</ul>

<b>Other Efficient Applications</b> Pre-trained models are used in many tasks in NLP that efficiency can be their concern.
<br>
<ul>
	<li>Efficient Dense Retrieval</li>
	<li>Large language model as a service</li>
	<li>Training models on device</li>
	<li>Incorporating external knowledge into pre-trained models</li>
	<li>Unifying different pre-training models</li>
</ul>

<br>

<h2 class="blackpar_title">Submission Instructions</h2>
<p>
You are invited to submit your papers in our CMT submission <a href="https://cmt3.research.microsoft.com/ENLSP2022">portal</a>. All the submitted papers have to be anonymous for double-blind review. We expect each paper will be reviewed by at least three reviewers. The content of the paper (excluding the references and supplementary materials) should not be longer than 4 pages, strictly following the NeurIPS template style (which can be found <a href="https://neurips.cc/Conferences/2022/PaperInformation/StyleFiles">here</a>). 
<br /><br />
Authors can submit up to 100 MB of supplementary materials separately. Authors are highly encouraged to submit their codes for reproducibility purposes. According to the guideline of the NeurIPS workshops, already published papers are not encouraged for submission, but you are allowed to submit your ArXiv papers or the ones which are under submission. Moreover, a work that is presented at the main NeurIPS conference should not appear in a workshop. Please make sure to indicate the complete list of conflict of interests for all the authors of your paper. To encourage higher quality submissions, our sponsors are offering the <b>Best Paper</b> and the <b>Best Poster</b> Award to qualified outstanding original oral and poster presentations (upon nomination of the reviewers). Also, we will give one <b>outstading paper certification</b> for our special track of efficient graph learning for NLP.Bear in mind that our workshop is not archival, but the accepted papers will be hosted on the workshop website.
 

</p>

<br>

<h2 class="blackpar_title">Important Dates:</h2>
<p>
<ul>
	<li>Submission Deadline: 5 July, 2023 AOE </li>
	<li>Acceptance Notification: 30 July, 2023 AOE </li>
	<li>Acceptance Notification: 30 July, 2023 AOE </strike> </li>	
	<li>Camera-Ready Submission: 6 August, 2023 AOE </li>
	<li>Workshop Date: <b>29 October - 3 November, 2023</b> (Attendance model: TBA)</li>
</ul>
</p> -->

<!--Confirmed Speakers-->
<h2 class="blackpar_title" id="speakers">Confirmed Speakers</h2>
<p>
TBA
<!-- {% include speakers.html %} -->
</p>


<!-- <h2 class="blackpar_title" id="speakers">Industrial Panelists</h2>
<p>
TBA
<!-- {% include panelists.html %}
</p> -->

<!-- Schedule -->
<h2 class="blackpar_title" id="schedule">Schedule (TBA)</h2>
<!-- <p>
{% include schedule.html %}
</p> -->

<!-- Organizers -->
<h2 class="blackpar_title" id="organizers">Organizers</h2>
<p>
{% include organizers.html %}
</p>

<!-- <h2 class="blackpar_title" id="Organizers">Volunteers</h2>
<div class="row_perso">
	<div class="card_perso column_perso justify-content-center" style="margin-left:24%;">
	  <img src="/images/khalil_bibi.png" alt="Khalil Bibi" class="img_card_perso">
	  <div class="container_perso" >
		<center>
		<h6>
			<b>Khalil Bibi</b>
			<br>
			Huawei Noah's Ark Lab
		</h6>
		</center>
	  </div>
	</div>
	<div class="card_perso column_perso">
	  <img src="/images/Soheila.png" alt="Soheila Samiee" class="img_card_perso">
	  <div class="container_perso">
		<center>
		<h6>
			<b>Soheila Samiee</b>
			<br>
			BASF
		</h6>
		</center>
	  </div>
	</div>
</div> -->


<br><br>

<!-- Technical Committee -->
<h2 class="blackpar_title" id="program_committee">Program Committee</h2>
<p>
TBA
<!-- {% include program_committee.html %} -->
</p>
<br><br>

<h2 class="blackpar_title">Sponsors</h2>
<div class="row">
	<div class="col">
		<center>
			<img src="/2023/images/ign.png">
		</center>
	</div>
	<div class="col">
		<center>
			<img src="/2023/images/TM-logo.png" width="250px">
		</center>
	</div>
	<div class="col">
		<center>
			<img src="/2023/images/logo_zhejiang.png" width="250px">
		</center>
	</div>
</div>

<!-- <h2 class="blackpar_title">Gold Sponsor</h2>
<div class="row">
	<div class="col">
		<center>
			<img src="/2023/images/BASF_logo.png" width="250px">
		</center>
	</div>
	<div class="col">
		<center>
			<img src="/2023/images/rbc_logo.svg" width="250px">
		</center>
	</div>
</div> -->

<!-- Technical Committee -->
<h2 class="blackpar_title" id="previous_editions">Previous Editions</h2>
<p>
{% include previous_editions.html %}
</p>